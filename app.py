import streamlit as st
import ollama
import io # ç”¨äºå¤„ç†æ–‡ä»¶å†…å®¹
import base64
import PyPDF2 
# æ–°å¢å¯¼å…¥çŸ¥è¯†åº“æ¨¡å—
from knowledge_base import add_to_knowledge_base, search_knowledge_base, OllamaEmbeddings
import os
from langchain_chroma import Chroma
from langchain.text_splitter import CharacterTextSplitter

# åˆå§‹åŒ– embedding æ¨¡å‹
embeddings = OllamaEmbeddings(model="bge-m3:latest")

# çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„
VECTOR_DB_PATH = "chroma_knowledge_base"

def build_knowledge_base(texts, metadatas=None):
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.create_documents(texts, metadatas=metadatas)
    db = Chroma.from_documents(docs, embeddings, persist_directory=VECTOR_DB_PATH)
    # db.persist() # Chroma 0.4.x åè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨

def load_knowledge_base():
    if not os.path.exists(VECTOR_DB_PATH):
        return None
    return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)

def add_to_knowledge_base(texts, metadatas=None):
    db = load_knowledge_base()
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    if metadatas is not None:
        docs = text_splitter.create_documents(texts, metadatas=metadatas * len(texts) if len(metadatas) == 1 else metadatas)
    else:
        docs = text_splitter.create_documents(texts)
    if db is None:
        build_knowledge_base(texts, metadatas)
    else:
        db.add_documents(docs)
        # db.persist() # Chroma 0.4.x åè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨

def search_knowledge_base(query, top_k=3):
    db = load_knowledge_base()
    if db is None:
        return []
    docs_and_scores = db.similarity_search_with_score(query, k=top_k)
    return [doc.page_content for doc, score in docs_and_scores]

def delete_by_filename(filename):
    db = load_knowledge_base()
    if db is None:
        return
    db.delete(where={"filename": filename})
    # db.persist() # Chroma 0.4.x åè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨

def list_all_filenames():
    db = load_knowledge_base()
    if db is None:
        return []
    all_docs = db.get()
    filenames = set()
    for meta in all_docs['metadatas']:
        if meta and 'filename' in meta:
            filenames.add(meta['filename'])
    return list(filenames)

# æ˜ç¡®æŒ‡å®šè¿æ¥åˆ°æœ¬åœ°çš„ Ollama æœåŠ¡
client = ollama.Client(host='http://localhost:11434') 

st.set_page_config(page_title="å¹¿è…¾AIåŠ©æ‰‹", layout="wide")

st.title("å¹¿è…¾AIåŠ©æ‰‹ ğŸ¤–")

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

temp_file_content = None # åœ¨é¡¶å±‚åˆå§‹åŒ–

# ä¾§è¾¹æ ç”¨äºæ–‡ä»¶ä¸Šä¼ å’ŒçŸ¥è¯†åº“ç®¡ç†
with st.sidebar:
    st.header("æ–‡ä»¶ä¸çŸ¥è¯†åº“ç®¡ç†")

    st.subheader("ğŸ“š ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶")
    kb_file = st.file_uploader("ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶ï¼ˆå†…å®¹å°†æ°¸ä¹…å­˜å…¥çŸ¥è¯†åº“ï¼‰", key="kb_upload", type=['txt', 'pdf'])
    if kb_file is not None:
        content = kb_file.read().decode("utf-8")
        add_to_knowledge_base([content], metadatas=[{"filename": kb_file.name}])
        st.success(f"æ–‡ä»¶ '{kb_file.name}' å·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼")

    st.subheader("ğŸ“ ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶")
    temp_file = st.file_uploader("ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶ï¼ˆä»…æœ¬æ¬¡å¯¹è¯å¯ç”¨ï¼Œä¸å…¥çŸ¥è¯†åº“ï¼‰", key="temp_upload", type=['txt', 'pdf'])
    if temp_file is not None:
        temp_file_content = temp_file.read().decode("utf-8")
        st.success(f"æ–‡ä»¶ '{temp_file.name}' å·²ä¸Šä¼ ï¼Œä»…æœ¬æ¬¡å¯¹è¯å¯ç”¨ã€‚")

    st.divider()

    with st.expander("ğŸ—‘ï¸ çŸ¥è¯†åº“ç®¡ç†", expanded=True):
        st.markdown("#### åˆ é™¤çŸ¥è¯†æ–‡ä»¶")
        all_filenames = list_all_filenames()
        selected_file = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„çŸ¥è¯†æ–‡ä»¶", all_filenames)
        if st.button("åˆ é™¤é€‰ä¸­æ–‡ä»¶çš„æ‰€æœ‰çŸ¥è¯†å†…å®¹"):
            from knowledge_base import delete_by_filename
            delete_by_filename(selected_file)
            st.success(f"å·²åˆ é™¤æ–‡ä»¶ '{selected_file}' ç›¸å…³çš„æ‰€æœ‰çŸ¥è¯†å†…å®¹ï¼")
        st.markdown("---")
        st.markdown("#### å½“å‰çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨")
        st.write(all_filenames if all_filenames else "æš‚æ— æ–‡ä»¶")

# èŠå¤©åŒºï¼ˆä¸»å†…å®¹åŒºåŸŸï¼‰
with st.container():
    st.subheader("ğŸ’¬ èŠå¤©åŒº")
    for message in st.session_state.get("messages", []):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è¾“å…¥ä½ çš„æ¶ˆæ¯..."):
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        # åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)

        # æ£€ç´¢çŸ¥è¯†åº“
        kb_contexts = search_knowledge_base(prompt, top_k=3)
        context_text = "\n".join(kb_contexts) if kb_contexts else ""
        # æ‹¼æ¥ä¸´æ—¶æ–‡ä»¶å†…å®¹
        if temp_file_content:
            context_text += f"\nã€ä¸´æ—¶æ–‡ä»¶å†…å®¹ã€‘\n{temp_file_content}"
        # æ„å»ºæœ€ç»ˆ prompt
        current_message_content = f"è¯·å‚è€ƒä»¥ä¸‹å†…å®¹ï¼š\n{context_text}\n\nç”¨æˆ·çš„é—®é¢˜/æŒ‡ä»¤æ˜¯ï¼š{prompt}"
        st.info("å·²ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹å¹¶æ·»åŠ åˆ°æç¤ºã€‚")

        # æ„å»ºå‘é€ç»™ Ollama çš„æ¶ˆæ¯
        messages_payload = [
            {'role': m['role'], 'content': m['content']}
            for m in st.session_state.messages[:-1]
        ] # å‘é€é™¤äº†å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰å†å²æ¶ˆæ¯

        # æ„å»ºå‘é€ç»™ Ollama çš„ messages åˆ—è¡¨
        message_to_send = {'role': 'user', 'content': current_message_content}

        messages_payload.append(message_to_send)

        # è°ƒç”¨ Ollama æ¨¡å‹ç”Ÿæˆå“åº”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                # ä½¿ç”¨ client å¯¹è±¡è°ƒç”¨ chat æ–¹æ³•
                stream = client.chat(
                    model='qwen2.5vl:32b', # Ollama æ¨¡å‹åç§°
                    messages=messages_payload,
                    stream=True,
                )
                for chunk in stream:
                    if chunk['message']['content'] is not None:
                        full_response += chunk['message']['content']
                        message_placeholder.markdown(full_response + "â–Œ") # æ˜¾ç¤ºæµå¼å“åº”

                message_placeholder.markdown(full_response) # æ˜¾ç¤ºæœ€ç»ˆå“åº” 

            except Exception as e:
                 st.error(f"è°ƒç”¨ Ollama æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                 full_response = "æ— æ³•è·å–å“åº”ã€‚"

            # å°†åŠ©æ‰‹çš„å“åº”æ·»åŠ åˆ°èŠå¤©å†å²
            st.session_state.messages.append({"role": "assistant", "content": full_response})

st.caption("Â© 2024 å¹¿è…¾AIåŠ©æ‰‹ | Powered by Streamlit & Ollama")

